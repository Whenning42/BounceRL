Project:
- AoR
  - GameHarness + Pixels + Easy Env
    - DrQv2
    - TODO: Document results
- Noita
  - GameHarness + Pixels + Harder Env
    - SB3 + tuple env + image regularization
        âœ“ Update SB3 to support tuple action space
          - Add unit test
        - Write SB3 Noita train script
        - Run unregularized train experiment
          - Default LR, 1e6 steps, PPO (lower sample efficiency than SAC, but more robust)
        - Update SB3 w/ image regularization
- Factorio

Experiment Log:

Noita SB3 PPO Exp 1 (no image regularization)
Path: out/noita_sb3_ppo_no_reg/

Unprioritized:

- Give a write-up (Something relatively light)
    - Documentation
    - Short reflection (Below is probably too much)
        - Motivation
            - Wanted to re-create OpenAI Universe.
            - Wanted to either succeed in training an AI to play a game,
              or learn what the "blockers" were
        - Game choice
            - Wrote up a list
            - Initially chose Skyrogue
        - Harness
        - Many side-tracks
            - MC
            - Compression
            - ...
        - Return to the project
            - A better focus on engineering instead of research
        - Reward Fns
        - Time accel
        - RL
        - Future directions
            - Solve AOR single track (assuming search doesn't)
            - Solve other AOR tracks
            - Solve other games

        - Distill the story above into a clear narrative
            - Why I built
            - What I built
            - How I grew
            - What's the state of RL
- Noita
    - DrQv2 + thresholding?
        - I don't this this would work, but would be interesting to try
        - Map (0, 1) via sigmoid of 4*tanh(2x-1) to get a smoothed (0, 1) prob
          - With this prob, sample the action space and use this action
          - There's not really a gradient here.